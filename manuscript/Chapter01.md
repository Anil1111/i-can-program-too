# Major Advancements in Computing

The history of computing starts a little over 80 years ago.  Mathematicians and theorists were speculating about the possibility to using electrical devices to solve problems in the mid 1930s.  By the end of that decade, computers were starting to be built.  These machines cost millions of dollars to construct (in today's money) and took up the space of several large rooms.  These machines could not do much more than solve basic mathematical problems.

The early computers were mostly used to perform mathematical calculations for governments by calculating bomb trajectories and decrypting enemy communications.  In the 1950s, computers slowly moved into the mainstream with their adoption by businesses for storing and processing data.  The 1960s brought about the rise of the operating system and the use of high-level computer languages.  Multi-user systems also started to become prevalent.  The 1970s laid the foundation for networking computers together and allowing many users to work on the same computer systems.  The 1980s brought computers into the home, while the 1990s brought us together with the Internet.  Let's see what major advancements brought us here today.

The first computers were single purpose machines.  They were each used for one purpose at a time.  For example, these computers could only calculate simple mathematical problems.  In order to change they type of calculation they performed, one would have to physically rewire the machine to enable it to perform another function.  These machines could perform tens of operations per second.  Today's smartphones are able to perform billions of operations per second.

The major breakthrough was to create a general purpose computer that could be used to solve more than one problem.  The idea was to create a machine that could be programmed by means other than physically changing the circuitry.  The first computer programs were written in machine language, that is, the language that a computer can understand.  This is represented by ones and zeroes.  Each computer had its own language and method of creating the input.

The earliest computers could be programmed by using switches on their consoles.  Computers at the time did not have monitors or even keyboards.  The only way to interact with the earliest computers was to flip switches on a big board.  To alleviate the problem of not being able to save programs, punch cards were used as a type of computer input.  A punch card is a stiff piece of paper divided into rows and columns.  Computer instructions were created by using combinations of holes created on the cards.  Computer operators would create punch cards on key punch machines, which were large keyboards that punched holes into these cards.  Thousands of cards would be created to create a program to run.

Computers still did not have monitors or connected keyboards.  Arrays of light bulbs were used to indicate the output from a program.  It wasn't long until assembly languages overtook raw machine languages.  Instead of using ones and zeroes, the programmer would use a more human-friendly way creating a program.  These programs, like ones in machine language, gave instructions to the computer about how to perform a computation.  Each line of code instructs the computer's processor on how to perform a calculation, for instance.  Even modern processors are not able to perform advanced tasks on their own.  It is up to the software to instruct the processor how to operate on data.

Assembly languages are used to this day to perform low-level operations to interact with hardware.  They are mainly seen in operating system kernels and hardware drivers (more on this later).

Assembly by itself cannot be run by the computer.  There is a program called an assembler that converts the assembly language to machine language.  In the end, all software is somehow converted down to machine language in order to be run.  Assembly is a language that is unique to each computer platform.  Assembly that was written for the earliest computers will not run on a modern computer.  Programmers of the day were specialists in the particular computer system they were programming.  Even different computer models from the same company generally had different dialects of assembly language.

As computers became more powerful and had more memory available to them, higher level languages were developed.  These languages did not specify each instruction that the computer had to perform.  Instead of requiring 10 or more commands to add two numbers together, they only required one or two.  Of these early compiled languages, COBOL and FORTRAN, which have their origins in the 1950s are still in use today.

A computer cannot understand the language as it is written.  It must first be sent through a program called a compiler.  The compiler translates the code to machine language, which allows it to be run by the computer.  These languages were easier to write than assembly.  They were also less error prone and helped to increase development times.

COBOL and FORTRAN were adopted by many different companies for the use on many different machine architectures.  This may seem to be a good thing to have languages that can run on multiple types of machines.  Unfortunately, the wide variety of operating and hardware systems had their own dialects of these languages.  One example I saw of a COBOL implementation was tighly coupled with the data types used by a specific system.  No other systems would be able to interact with this implementation.



PORTABLE LANGUAGES 

UNIX
